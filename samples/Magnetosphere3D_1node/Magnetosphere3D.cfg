# Sample Vlasiator configuration file
# Low-resolution magnetosphere with two mesh refinement levels (total 3 resolution levels)
# This simulation can run on a single 40-GiB GPU in a handful of hours.
# Suitable for simple benchmarking of performance
# GPU runs with larger blocks (WID=8) may need to reduce the number of temporary GPUallocations
# in order to fit in unified memory on-device without swapping. (e.g. --GPUallocations=24 on A100 40GiB)

# Reference benchmark information from August 2025 (2cfb2c64f931eda7c7274acfca31b53b471004b5)
#                     Mahti CPU 32 cores, WID=4:
# Init                45 s
# propagate           5393 s
# spatial-space       3351 s
# velocity space      1682 s
# fieldsolver         228.9 s
#                     Mahti CPU 32 cores, WID=8:
# Init                47 s
# propagate           6834 s
# spatial-space       3691 s
# velocity space      2720 s
# fieldsolver         219 s
#                     LUMI-C 32 cores, WID=4:
# Init                40 s
# propagate           4520 s
# spatial-space       2602 s
# velocity space      1632 s
# fieldsolver         175.7 s
#                     LUMI-C 32 cores, WID=8:
# Init                41 s
# propagate           6175 s
# spatial-space       3299 s
# velocity space      2521 s
# fieldsolver         178.9 s

#                     Mahti A100, WID=4:
# Init                206 s
# propagate           1451 s
# spatial-space       313.8 s
# velocity space      827.9 s
# fieldsolver         285.3 s
#                     Mahti A100, WID=8:
# Init                257 s
# propagate           2594 s
# spatial-space       593.3 s
# velocity space      1699 s
# fieldsolver         278.1 s

#                     LUMI-G single MI250X GCD, WID=4:
# Init                136 s
# propagate           3806 s
# spatial-space       1630 s
# velocity space      1325 s
# fieldsolver         812 s (only using 6 CPU cores)
#                     LUMI-G single MI250X GCD, WID=8:
# Init                134 s
# propagate           3865 s
# spatial-space       1011 s
# velocity space      2018 s
# fieldsolver         804 s (only using 6 CPU cores)

# The Mahti multi-GPU performance results are limited by GPU-aware MPI issues:
#                     Mahti 4 x A100 (1 node), WID=4:
# Init                210.4 s
# propagate           3013 s
# spatial-space       1158 s
# velocity space      856.5 s
# fieldsolver         253.7 s
#                     Mahti 4 x A100 (1 node), WID=8:
# Init                213.4 s
# propagate           3715 s
# spatial-space       1899 s
# velocity space      850.1 s
# fieldsolver         249.2 s

#                     LUMI-G 8x MI250X GCD (1 node), WID=4:
# Init                54.3 s
# propagate           1682 s
# spatial-space       1032 s
# velocity space      212.8 s
# fieldsolver         211.1 s (only using 6 CPU cores per task)
#                     LUMI-G 8x MI250X GCD (1 node), WID=8:
# Init                55.7 s
# propagate           2931 s
# spatial-space       2109 s
# velocity space      216.4 s
# fieldsolver         212.4 s (only using 6 CPU cores per task)


# After a handful of top-level options, remaining options are grouped in sections prefixed by [group] in this file.

# Simulation type, corresponding to a specific type of setup and a dedicated class defined under projects/ in the source
project = Magnetosphere

# Let the simulation adapt the time step according to the CFL conditions
dynamic_timestep = 1

# Arbitrary name given to at least one ion population. See [proton_properties] for its specification.
# all option groups specific to this population will be prefixed with that name, i.e. proton_* in this case.
ParticlePopulations = proton

GPUallocations = 256 # increase count of GPU allocations

# Set of options specifying conditions under which the run will exit gracefully to avoid potential crashing.
[bailout]
max_memory = 200 # set based on system

# Adaptive Mesh Refinement
[AMR]
max_spatial_level = 2
#refine_radius = 5e7 #2e8

# Input/Output
[io]
diagnostic_write_interval = 10
write_initial_state = 0
# restart_walltime_interval = 21000
# write_restart_stripe_factor = 42
# number_of_restarts = 12
# vlsv_buffer_size = 0
# write_as_float = 1

# system_write_t_interval = 10
# system_write_file_name = bulk
# system_write_distribution_stride = 0
# system_write_distribution_xline_stride = 10
# system_write_distribution_yline_stride = 10
# system_write_distribution_zline_stride = 10

# system_write_distribution_shell_radius = 31e6
# system_write_distribution_shell_stride = 1

# Spatial grid and run completion parameters
[gridbuilder] # Highest resolution 2250 km cubed (see [AMR])
x_length = 26
y_length = 20
z_length = 20
x_min = -6.3e8
x_max = 3.06e8
y_min = -3.6e8
y_max = 3.6e8
z_min = -3.6e8
z_max = 3.6e8

#timestep_max = 175
t_max = 20.0

# Specifications of ion population(s) declared at the top
[proton_properties]
mass = 1
mass_units = PROTON
charge = 1

# Velocity space parameters for ion population(s) declared at the top
[proton_vspace]
vx_min = -3.0e6
vx_max = +3.0e6
vy_min = -3.0e6
vy_max = +3.0e6
vz_min = -3.0e6
vz_max = +3.0e6
vx_length = 50 # in blocks of 4x4x4 v-space cells
vy_length = 50 # gives 30 km/s
vz_length = 50

# Velocity space sparsity parameters for ion population(s) declared at the top
[proton_sparse]
minValue = 1.0e-15
# dynamicAlgorithm = 0
# dynamicBulkValue1 = 1.0e6
# dynamicBulkValue2 = 1.0e7
# dynamicMinValue1 = 1.0e-15
# dynamicMinValue2 = 1.0e-13

# Options specific to the project
[Magnetosphere]
constBgBX = 0.0
constBgBY = 0.0
constBgBZ = -5.0e-9

dipoleType = 4
dipoleTiltPhi = 0.0 
dipoleTiltTheta = 0 
dipoleXFull = 9.5565e7 # 15 RE 
dipoleXZero = 2.5e8 
dipoleInflowBX = 0.0 
dipoleInflowBY = 0.0 
dipoleInflowBZ = 0.0 

# Forced refinement regions
refine_L2radius = 1.4e8
refine_L2tailthick = 0
refine_L1radius = 1.8e8
refine_L1tailthick = 0

# Options specific to the project and the ion population
[proton_Magnetosphere]
T = 0.5e6
rho = 1.0e6
VX0 = -7.5e5
VY0 = 0.0
VZ0 = 0.0
# Taper radius during which populations transitions from ionospheric parameters to magnetospheric parameters.
# Usually the inner radius is the same as the inner boundary, here it is increased to have more hot plasma in the inner region.
taperInnerRadius = 5e7
taperOuterRadius = 1e8

# Simulation boundary conditions; periodicity and declaration of the boundary types that will be required
[boundaries]
periodic_x = no
periodic_y = no
periodic_z = no
boundary = Outflow
boundary = Maxwellian
boundary = Copysphere

# Outflow options
# Precedence sets the order of precedence for overlapping boundary types, should not be changed.
[outflow]
precedence = 3

[proton_outflow]
face = x-
face = y-
face = y+
face = z-
face = z+

# Uniform Maxwellian distribution on set face(s) of the simulation domain
[maxwellian]
face = x+
precedence = 4
reapplyUponRestart = 1

[proton_maxwellian] # inflow file
dynamic = 0
file_x+ = sw1.dat

# Spherical inner boundary with static velocity distribution and copy-condition for perturbed magnetic field
[copysphere]
centerX = 0.0
centerY = 0.0
centerZ = 0.0
radius = 38.1e6
precedence = 2
reapplyUponRestart = 1

[proton_copysphere]
# increased temperature and density close to inner boundary to add load balance challenge
T = 1.5e6
rho = 3.0e6
VX0 = 0.0
VY0 = 0.0
VZ0 = 0.0

# MPI domain load balancing options
[loadBalance]
algorithm = RCB # RIB or HYPERGRAPH may yield good performance too
# This times the approximate timestep length should be about the same or less than the fastest
# dynamics so that the computations are well-distributed across MPI ranks.
rebalanceInterval = 50
# Closer to 1.0 -> more even distribution -> but it takes longer to obtain the load balance from Zoltan
tolerance = 1.2

# Variables (data reduction operators) to be written to the output VLSV files and to the diagnostic.txt file
# fg_* variables on the finest-resolution field solver grid (/!\ can be several GB per component in large runs)
# vg_* variables on the adaptive mesh storing the plasma distribution and moments
# populations_* variables are stored separately for each specified ion population as <population>/vg_*
[variables]
#output = fg_rank
#output = fg_b
#output = fg_b_perturbed
#output = fg_b_vol
#output = fg_boundarytype
#output = fg_e
#output = fg_e_hall
#output = fg_maxdt_fieldsolver
#output = fg_rhom
output = populations_vg_rho
output = populations_vg_v
output = populations_vg_ptensor
output = populations_vg_blocks
output = populations_vg_effectivesparsitythreshold
output = populations_vg_energydensity
output = populations_vg_precipitationdifferentialflux
#output = populations_vg_maxdt_acceleration
#output = populations_vg_maxdt_translation
#output = populations_vg_rho_loss_adjust
output = vg_b_vol
output = vg_e_vol
output = vg_e_gradpe
output = vg_boundarytype
output = vg_f_saved
output = vg_loadbalance_weight
output = vg_rank
diagnostic = populations_vg_blocks
#diagnostic = populations_vg_rho
#diagnostic = populations_vg_rho_loss_adjust

# Options for the population-specific energy density data reducer
[proton_energydensity]
solarwindspeed = 7.5e5

# Options for the population-specific precipitation data reducer
[proton_precipitation]
nChannels = 9
emin = 500     # These are eV
emax = 50000

# Field solver options
[fieldsolver]
maxSubcycles = 50
ohmHallTerm = 2
minCFL = 0.4
maxCFL = 0.45
maxWaveVelocity = 7494811.45  #2.5% of speed of light...    

electronTemperature = 0.5e6 # inflow
electronDensity = 1.0e6 #inflow
electronPTindex = 1.666667 # adiabatic
ohmGradPeTerm = 1 # active

# Vlasov solver (translation and acceleration) options
[vlasovsolver]
minCFL = 0.8
maxCFL = 0.99
# The angle x number of subcycles should not exceed 45 deg
maxSlAccelerationRotation = 22 # in deg
maxSlAccelerationSubcycles = 2

